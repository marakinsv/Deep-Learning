{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2HgFToyRiQJ9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Функция, реализующая код Цезаря.**"
      ],
      "metadata": {
        "id": "dEEUNssOjEtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция шифрования с тремя параметрами: текст, ключ, язык\n",
        "def ceaser_cipher(user, key, lang):\n",
        "    # Переменная результата шифрования; переменная, определяющая верхний и нижний регистр\n",
        "    res, n = [], \"\"\n",
        "\n",
        "    # Проверка пользователем выбранного языка\n",
        "\n",
        "    # Проверка выбран ли русский язык (регистр букв, вводимых пользователем, не важен)\n",
        "    if lang.lower() in [\"русский\", \"russian\"]:\n",
        "        # Двум переменным присваиваются русская азбука нижнего и верхнего регистра соответственно\n",
        "        dictionary, dictionary_upper = \"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\", \"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ\"\n",
        "    # Проверка выбран ли английский язык язык (регистр букв, вводимых пользователем, не важен)\n",
        "    elif lang.lower() in [\"английский\", \"english\"]:\n",
        "        # Двум переменным присваиваются английской азбука нижнего и верхнего регистра соответственно\n",
        "        dictionary, dictionary_upper = \"abcdefghijklmnopqrstuvwxyz\", \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "    else:\n",
        "        return \"Такого языка нет в опции\"\n",
        "\n",
        "    # Цикл проверки, где каждую итерацию будет обрабатываться один символ из текста последовательно\n",
        "    for i in range(len(user)):\n",
        "        # Проверка символа на верхний или нижний регистр\n",
        "\n",
        "        # Принадлежит ли символ нижнему регистру\n",
        "        if user[i] in dictionary:\n",
        "            n = dictionary\n",
        "        # Принадлежит ли символ верхнему регистру\n",
        "        elif user[i] in dictionary_upper:\n",
        "            n = dictionary_upper\n",
        "        # Символ не принадлежит ни нижнему ни верхнему регистру (символ не является буквой)\n",
        "        else:\n",
        "            res.append(user[i])\n",
        "\n",
        "        # Если символ есть в списке n (является буквой), то будет происходить его зашифровка\n",
        "        if user[i] in n:\n",
        "            # Цикл перебора азбуки\n",
        "            for j in range(len(n)):\n",
        "                # Если порядковый номер буквы + ключ находятся  в диапазоне от 0 до конца азбуки\n",
        "                # и если буква из текста совпадает с буквой из азбуки, то:\n",
        "                if 0 <= j + key < len(n) and user[i] == n[j]:\n",
        "                    # В результат добавляется буква со сдвигом key (зашифрованная буква)\n",
        "                    res.append(n[j + key])\n",
        "                # Если порядковый номер буквы + ключ выходит из диапазона азбуки, превышая его\n",
        "                # и если буква из текста совпадает с буквой из азбуки, то:\n",
        "                elif j + key >= len(n) and user[i] == n[j]:\n",
        "                    # В результат добавляеться буква со сдвигом key,\n",
        "                    # при этом преводя порядковый номер буквы к диапазону азбуки (зашифрованая буква)\n",
        "                    res.append(n[(1 - j - key) % (len(n) - 1)])\n",
        "                # Если порядковый номер буквы + ключ выходит из диапазона азбуки, недотягивает до него\n",
        "                # и если буква из текста совпадает с буквой из азбуки, то:\n",
        "                elif j + key < 0 and user[i] == n[j]:\n",
        "                    # В результат добавляется буква со сдвигом key,\n",
        "                    # при этом приводя порядковый номер буквы к диапазону азбуки (зашифрованная буква)\n",
        "                    res.append(n[(j + key) % len(n)])\n",
        "\n",
        "    # Функция возвращает зашифрованный текст\n",
        "    return ''.join(res)"
      ],
      "metadata": {
        "id": "ZG3mkdkUiQzB"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загружаем текст для обучения модели.**"
      ],
      "metadata": {
        "id": "sK2U8SsIq9K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pushkin_stihi2.txt', encoding='cp1251') as f:\n",
        "    text_orig = f.read().lower()\n",
        "\n",
        "print('length:', len(text_orig))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2yWfIEBq71c",
        "outputId": "d9883bae-4339-4a26-983f-f95bb290083c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length: 208312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "text_X = []\n",
        "\n",
        "kid = 0\n",
        "for ph in text_orig.split('\\n'):\n",
        "    if not (type(ph) is str): continue\n",
        "    ls = []\n",
        "    for c in re.sub('\\s+', ' ', ceaser_cipher(ph.strip(), key=keys[kid], lang=\"русский\")):\n",
        "        if c in ['.', ',', '-', ':', ';', '\"', '!', '?']: continue\n",
        "        ls.append(c)\n",
        "    text_X.append(ls)\n",
        "\n",
        "    kid += 1\n",
        "\n",
        "    if kid >= len(keys): kid = 0\n",
        "\n",
        "text_X[23]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG-YdB0b3GkB",
        "outputId": "f4dee941-a248-4be9-a256-d0264af0c780"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ц',\n",
              " 'х',\n",
              " 'н',\n",
              " 'п',\n",
              " 'и',\n",
              " 'п',\n",
              " 'с',\n",
              " 'ж',\n",
              " ' ',\n",
              " 'ф',\n",
              " 'ж',\n",
              " ' ',\n",
              " 'у',\n",
              " 'х',\n",
              " 'л',\n",
              " 'у',\n",
              " ' ',\n",
              " 'ц',\n",
              " 'х',\n",
              " 'к',\n",
              " 'и',\n",
              " 'х',\n",
              " 'ч',\n",
              " 'ь',\n",
              " 'л']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_y = [[c for c in re.sub('\\s+', ' ', ph.strip()) if not (c in ['.', ',', '-', ':', ';', '\"', '!', '?'])] for ph in text_orig.split('\\n') if type(ph) is str]\n",
        "\n",
        "text_y[23]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijG8DVDAxuwp",
        "outputId": "c2c93e37-68cf-4c1b-8320-d71deab5eef8"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['п',\n",
              " 'о',\n",
              " 'ж',\n",
              " 'и',\n",
              " 'в',\n",
              " 'и',\n",
              " 'к',\n",
              " 'а',\n",
              " ' ',\n",
              " 'н',\n",
              " 'а',\n",
              " ' ',\n",
              " 'м',\n",
              " 'о',\n",
              " 'е',\n",
              " 'м',\n",
              " ' ',\n",
              " 'п',\n",
              " 'о',\n",
              " 'д',\n",
              " 'в',\n",
              " 'о',\n",
              " 'р',\n",
              " 'ь',\n",
              " 'е']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Делаем массив с данными"
      ],
      "metadata": {
        "id": "wxdmrCvIzZT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 30"
      ],
      "metadata": {
        "id": "HyPzvzcz0rgU"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHARS = list('абвгдеёжзийклмнопрстуфхцчшщъыьэюя ')\n",
        "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]\n",
        "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}"
      ],
      "metadata": {
        "id": "Mw_RLWOqzapZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.zeros((len(text_X), MAX_LEN), dtype=int)\n",
        "for i in range(len(text_X)):\n",
        "    for j, w in enumerate(text_X[i]):\n",
        "        if j >= MAX_LEN:\n",
        "            break\n",
        "        X[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])\n",
        "\n",
        "y = torch.zeros((len(text_y), MAX_LEN), dtype=int)\n",
        "for i in range(len(text_y)):\n",
        "    for j, w in enumerate(text_y[i]):\n",
        "        if j >= MAX_LEN:\n",
        "            break\n",
        "        y[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
      ],
      "metadata": {
        "id": "TJCzpf1hzf1k"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[23:25], y[23:25]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWJQ9u0vz7Gs",
        "outputId": "8f8d87f6-e3c9-4384-937d-b34eb4efdc50"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[24, 23, 15, 17, 10, 17, 19,  8, 34, 22,  8, 34, 21, 23, 13, 21, 34, 24,\n",
              "          23, 12, 10, 23, 25, 30, 13,  0,  0,  0,  0,  0],\n",
              "         [24, 20,  9, 16, 18, 34, 27, 11, 24, 14, 34, 29, 27, 14, 26, 13, 18, 14,\n",
              "          34, 18, 34, 25, 26, 24, 11, 24, 26, 29, 14,  0]]),\n",
              " tensor([[17, 16,  8, 10,  3, 10, 12,  1, 34, 15,  1, 34, 14, 16,  6, 14, 34, 17,\n",
              "          16,  5,  3, 16, 18, 30,  6,  0,  0,  0,  0,  0],\n",
              "         [16, 12,  1,  8, 10, 34, 19,  3, 16,  6, 34, 21, 19,  6, 18,  5, 10,  6,\n",
              "          34, 10, 34, 17, 18, 16,  3, 16, 18, 30,  6,  0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Модель.**"
      ],
      "metadata": {
        "id": "m9upKREW1ueL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.embed = torch.nn.Embedding(len(CHAR_TO_INDEX), 28)\n",
        "        self.rnn = torch.nn.RNN(28, 128, batch_first=True)\n",
        "        self.linear = torch.nn.Linear(128, len(INDEX_TO_CHAR))\n",
        "        \n",
        "    def forward(self, sentences, state=None):\n",
        "        embed = self.embed(sentences)\n",
        "        o, s = self.rnn(embed)\n",
        "        out = self.linear(o)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Zs1UzFkV1yPH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Network()\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
      ],
      "metadata": {
        "id": "AouuEMLV126h"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Обучаем модель.**"
      ],
      "metadata": {
        "id": "xUmF9qJDW8Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(900):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "\n",
        "    for i in range(int(len(X) / 100)):\n",
        "        X_batch = X[i * 100:(i + 1) * 100]\n",
        "        Y_batch = y[i * 100:(i + 1) * 100].flatten()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        answers = model(X_batch)\n",
        "        answers = answers.view(-1, len(INDEX_TO_CHAR))\n",
        "        loss = criterion(answers, Y_batch)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1\n",
        "\n",
        "     \n",
        "\n",
        "    if ep%5 == 0: \n",
        "        print(\"\\nEpoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
        "        #s = generate_sentence()\n",
        "        #print(s)\n",
        "    else:\n",
        "        print(f\"\\rEpoch {ep}, loss: {train_loss / train_passed:.3f}\", end='') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97XrRxuW1_rN",
        "outputId": "838e9bdf-8bb3-49e3-e5d0-f24323939e8c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0. Time: 1.295, Train loss: 2.487\n",
            "Epoch 4, loss: 1.713\n",
            "Epoch 5. Time: 1.208, Train loss: 1.649\n",
            "Epoch 9, loss: 1.457\n",
            "Epoch 10. Time: 1.250, Train loss: 1.422\n",
            "Epoch 14, loss: 1.328\n",
            "Epoch 15. Time: 1.199, Train loss: 1.312\n",
            "Epoch 19, loss: 1.265\n",
            "Epoch 20. Time: 1.214, Train loss: 1.256\n",
            "Epoch 24, loss: 1.224\n",
            "Epoch 25. Time: 1.204, Train loss: 1.216\n",
            "Epoch 29, loss: 1.165\n",
            "Epoch 30. Time: 1.207, Train loss: 1.150\n",
            "Epoch 34, loss: 1.101\n",
            "Epoch 35. Time: 1.222, Train loss: 1.091\n",
            "Epoch 39, loss: 1.052\n",
            "Epoch 40. Time: 1.199, Train loss: 1.044\n",
            "Epoch 44, loss: 1.012\n",
            "Epoch 45. Time: 1.192, Train loss: 1.005\n",
            "Epoch 49, loss: 0.978\n",
            "Epoch 50. Time: 1.206, Train loss: 0.971\n",
            "Epoch 54, loss: 0.947\n",
            "Epoch 55. Time: 1.209, Train loss: 0.941\n",
            "Epoch 59, loss: 0.919\n",
            "Epoch 60. Time: 1.185, Train loss: 0.913\n",
            "Epoch 64, loss: 0.890\n",
            "Epoch 65. Time: 1.399, Train loss: 0.885\n",
            "Epoch 69, loss: 0.863\n",
            "Epoch 70. Time: 1.192, Train loss: 0.857\n",
            "Epoch 74, loss: 0.836\n",
            "Epoch 75. Time: 1.695, Train loss: 0.831\n",
            "Epoch 79, loss: 0.810\n",
            "Epoch 80. Time: 1.206, Train loss: 0.804\n",
            "Epoch 84, loss: 0.782\n",
            "Epoch 85. Time: 1.963, Train loss: 0.775\n",
            "Epoch 89, loss: 0.754\n",
            "Epoch 90. Time: 1.215, Train loss: 0.750\n",
            "Epoch 94, loss: 0.741\n",
            "Epoch 95. Time: 1.904, Train loss: 0.723\n",
            "Epoch 99, loss: 0.704\n",
            "Epoch 100. Time: 1.192, Train loss: 0.700\n",
            "Epoch 104, loss: 0.691\n",
            "Epoch 105. Time: 1.753, Train loss: 0.676\n",
            "Epoch 109, loss: 0.664\n",
            "Epoch 110. Time: 1.190, Train loss: 0.663\n",
            "Epoch 114, loss: 0.644\n",
            "Epoch 115. Time: 1.447, Train loss: 0.634\n",
            "Epoch 119, loss: 0.618\n",
            "Epoch 120. Time: 1.203, Train loss: 0.613\n",
            "Epoch 124, loss: 0.613\n",
            "Epoch 125. Time: 1.202, Train loss: 0.596\n",
            "Epoch 129, loss: 0.581\n",
            "Epoch 130. Time: 1.185, Train loss: 0.577\n",
            "Epoch 134, loss: 0.564\n",
            "Epoch 135. Time: 1.195, Train loss: 0.561\n",
            "Epoch 139, loss: 0.548\n",
            "Epoch 140. Time: 1.241, Train loss: 0.545\n",
            "Epoch 144, loss: 0.532\n",
            "Epoch 145. Time: 1.197, Train loss: 0.529\n",
            "Epoch 149, loss: 0.525\n",
            "Epoch 150. Time: 1.205, Train loss: 0.518\n",
            "Epoch 154, loss: 0.503\n",
            "Epoch 155. Time: 1.209, Train loss: 0.500\n",
            "Epoch 159, loss: 0.494\n",
            "Epoch 160. Time: 1.199, Train loss: 0.488\n",
            "Epoch 164, loss: 0.476\n",
            "Epoch 165. Time: 1.225, Train loss: 0.474\n",
            "Epoch 169, loss: 0.466\n",
            "Epoch 170. Time: 1.234, Train loss: 0.462\n",
            "Epoch 174, loss: 0.451\n",
            "Epoch 175. Time: 1.218, Train loss: 0.449\n",
            "Epoch 179, loss: 0.439\n",
            "Epoch 180. Time: 1.221, Train loss: 0.437\n",
            "Epoch 184, loss: 0.428\n",
            "Epoch 185. Time: 1.218, Train loss: 0.426\n",
            "Epoch 189, loss: 0.417\n",
            "Epoch 190. Time: 1.244, Train loss: 0.415\n",
            "Epoch 194, loss: 0.407\n",
            "Epoch 195. Time: 1.218, Train loss: 0.404\n",
            "Epoch 199, loss: 0.396\n",
            "Epoch 200. Time: 1.482, Train loss: 0.394\n",
            "Epoch 204, loss: 0.386\n",
            "Epoch 205. Time: 1.207, Train loss: 0.385\n",
            "Epoch 209, loss: 0.377\n",
            "Epoch 210. Time: 1.701, Train loss: 0.375\n",
            "Epoch 214, loss: 0.368\n",
            "Epoch 215. Time: 1.204, Train loss: 0.366\n",
            "Epoch 219, loss: 0.359\n",
            "Epoch 220. Time: 1.886, Train loss: 0.358\n",
            "Epoch 224, loss: 0.396\n",
            "Epoch 225. Time: 1.195, Train loss: 0.418\n",
            "Epoch 229, loss: 0.345\n",
            "Epoch 230. Time: 1.930, Train loss: 0.343\n",
            "Epoch 234, loss: 0.337\n",
            "Epoch 235. Time: 1.186, Train loss: 0.335\n",
            "Epoch 239, loss: 0.329\n",
            "Epoch 240. Time: 1.850, Train loss: 0.328\n",
            "Epoch 244, loss: 0.323\n",
            "Epoch 245. Time: 1.197, Train loss: 0.321\n",
            "Epoch 249, loss: 0.316\n",
            "Epoch 250. Time: 1.570, Train loss: 0.315\n",
            "Epoch 254, loss: 0.310\n",
            "Epoch 255. Time: 1.194, Train loss: 0.309\n",
            "Epoch 259, loss: 0.304\n",
            "Epoch 260. Time: 1.249, Train loss: 0.303\n",
            "Epoch 264, loss: 0.299\n",
            "Epoch 265. Time: 1.230, Train loss: 0.298\n",
            "Epoch 269, loss: 0.294\n",
            "Epoch 270. Time: 1.219, Train loss: 0.293\n",
            "Epoch 274, loss: 0.289\n",
            "Epoch 275. Time: 1.202, Train loss: 0.288\n",
            "Epoch 279, loss: 0.284\n",
            "Epoch 280. Time: 1.232, Train loss: 0.283\n",
            "Epoch 284, loss: 0.280\n",
            "Epoch 285. Time: 1.221, Train loss: 0.279\n",
            "Epoch 289, loss: 0.276\n",
            "Epoch 290. Time: 1.223, Train loss: 0.275\n",
            "Epoch 294, loss: 0.271\n",
            "Epoch 295. Time: 1.201, Train loss: 0.271\n",
            "Epoch 299, loss: 0.268\n",
            "Epoch 300. Time: 1.234, Train loss: 0.267\n",
            "Epoch 304, loss: 0.264\n",
            "Epoch 305. Time: 1.208, Train loss: 0.264\n",
            "Epoch 309, loss: 0.315\n",
            "Epoch 310. Time: 1.206, Train loss: 0.302\n",
            "Epoch 314, loss: 0.265\n",
            "Epoch 315. Time: 1.243, Train loss: 0.261\n",
            "Epoch 319, loss: 0.255\n",
            "Epoch 320. Time: 1.201, Train loss: 0.254\n",
            "Epoch 324, loss: 0.251\n",
            "Epoch 325. Time: 1.363, Train loss: 0.250\n",
            "Epoch 329, loss: 0.248\n",
            "Epoch 330. Time: 1.230, Train loss: 0.247\n",
            "Epoch 334, loss: 0.245\n",
            "Epoch 335. Time: 1.736, Train loss: 0.244\n",
            "Epoch 339, loss: 0.242\n",
            "Epoch 340. Time: 1.205, Train loss: 0.241\n",
            "Epoch 344, loss: 0.239\n",
            "Epoch 345. Time: 2.022, Train loss: 0.238\n",
            "Epoch 349, loss: 0.236\n",
            "Epoch 350. Time: 1.208, Train loss: 0.236\n",
            "Epoch 354, loss: 0.234\n",
            "Epoch 355. Time: 1.814, Train loss: 0.233\n",
            "Epoch 359, loss: 0.231\n",
            "Epoch 360. Time: 1.245, Train loss: 0.231\n",
            "Epoch 364, loss: 0.229\n",
            "Epoch 365. Time: 1.448, Train loss: 0.228\n",
            "Epoch 369, loss: 0.227\n",
            "Epoch 370. Time: 1.220, Train loss: 0.226\n",
            "Epoch 374, loss: 0.225\n",
            "Epoch 375. Time: 1.225, Train loss: 0.224\n",
            "Epoch 379, loss: 0.222\n",
            "Epoch 380. Time: 1.212, Train loss: 0.222\n",
            "Epoch 384, loss: 0.220\n",
            "Epoch 385. Time: 1.220, Train loss: 0.220\n",
            "Epoch 389, loss: 0.218\n",
            "Epoch 390. Time: 1.210, Train loss: 0.218\n",
            "Epoch 394, loss: 0.217\n",
            "Epoch 395. Time: 1.242, Train loss: 0.216\n",
            "Epoch 399, loss: 0.215\n",
            "Epoch 400. Time: 1.246, Train loss: 0.214\n",
            "Epoch 404, loss: 0.213\n",
            "Epoch 405. Time: 1.230, Train loss: 0.213\n",
            "Epoch 409, loss: 0.211\n",
            "Epoch 410. Time: 1.208, Train loss: 0.211\n",
            "Epoch 414, loss: 0.209\n",
            "Epoch 415. Time: 1.222, Train loss: 0.209\n",
            "Epoch 419, loss: 0.208\n",
            "Epoch 420. Time: 1.224, Train loss: 0.208\n",
            "Epoch 424, loss: 0.206\n",
            "Epoch 425. Time: 1.251, Train loss: 0.206\n",
            "Epoch 429, loss: 0.205\n",
            "Epoch 430. Time: 1.451, Train loss: 0.204\n",
            "Epoch 434, loss: 0.203\n",
            "Epoch 435. Time: 1.248, Train loss: 0.203\n",
            "Epoch 439, loss: 0.202\n",
            "Epoch 440. Time: 1.890, Train loss: 0.202\n",
            "Epoch 444, loss: 0.200\n",
            "Epoch 445. Time: 1.223, Train loss: 0.200\n",
            "Epoch 449, loss: 0.199\n",
            "Epoch 450. Time: 1.970, Train loss: 0.199\n",
            "Epoch 454, loss: 0.198\n",
            "Epoch 455. Time: 1.212, Train loss: 0.197\n",
            "Epoch 459, loss: 0.196\n",
            "Epoch 460. Time: 1.660, Train loss: 0.196\n",
            "Epoch 464, loss: 0.195\n",
            "Epoch 465. Time: 1.204, Train loss: 0.195\n",
            "Epoch 469, loss: 0.194\n",
            "Epoch 470. Time: 1.443, Train loss: 0.194\n",
            "Epoch 474, loss: 0.193\n",
            "Epoch 475. Time: 1.220, Train loss: 0.192\n",
            "Epoch 479, loss: 0.192\n",
            "Epoch 480. Time: 1.218, Train loss: 0.191\n",
            "Epoch 484, loss: 0.190\n",
            "Epoch 485. Time: 1.214, Train loss: 0.190\n",
            "Epoch 489, loss: 0.189\n",
            "Epoch 490. Time: 1.232, Train loss: 0.189\n",
            "Epoch 494, loss: 0.188\n",
            "Epoch 495. Time: 1.208, Train loss: 0.188\n",
            "Epoch 499, loss: 0.187\n",
            "Epoch 500. Time: 1.216, Train loss: 0.187\n",
            "Epoch 504, loss: 0.186\n",
            "Epoch 505. Time: 1.208, Train loss: 0.186\n",
            "Epoch 509, loss: 0.185\n",
            "Epoch 510. Time: 1.231, Train loss: 0.185\n",
            "Epoch 514, loss: 0.184\n",
            "Epoch 515. Time: 1.217, Train loss: 0.184\n",
            "Epoch 519, loss: 0.183\n",
            "Epoch 520. Time: 1.237, Train loss: 0.183\n",
            "Epoch 524, loss: 0.182\n",
            "Epoch 525. Time: 1.280, Train loss: 0.182\n",
            "Epoch 529, loss: 0.181\n",
            "Epoch 530. Time: 1.234, Train loss: 0.181\n",
            "Epoch 534, loss: 0.180\n",
            "Epoch 535. Time: 1.722, Train loss: 0.180\n",
            "Epoch 539, loss: 0.179\n",
            "Epoch 540. Time: 1.228, Train loss: 0.179\n",
            "Epoch 544, loss: 0.179\n",
            "Epoch 545. Time: 2.025, Train loss: 0.178\n",
            "Epoch 549, loss: 0.178\n",
            "Epoch 550. Time: 1.219, Train loss: 0.178\n",
            "Epoch 554, loss: 0.177\n",
            "Epoch 555. Time: 1.730, Train loss: 0.177\n",
            "Epoch 559, loss: 0.176\n",
            "Epoch 560. Time: 1.226, Train loss: 0.176\n",
            "Epoch 564, loss: 0.175\n",
            "Epoch 565. Time: 1.338, Train loss: 0.175\n",
            "Epoch 569, loss: 0.175\n",
            "Epoch 570. Time: 1.220, Train loss: 0.174\n",
            "Epoch 574, loss: 0.174\n",
            "Epoch 575. Time: 1.225, Train loss: 0.174\n",
            "Epoch 579, loss: 0.173\n",
            "Epoch 580. Time: 1.211, Train loss: 0.173\n",
            "Epoch 584, loss: 0.172\n",
            "Epoch 585. Time: 1.214, Train loss: 0.172\n",
            "Epoch 589, loss: 0.172\n",
            "Epoch 590. Time: 1.227, Train loss: 0.171\n",
            "Epoch 594, loss: 0.171\n",
            "Epoch 595. Time: 1.234, Train loss: 0.171\n",
            "Epoch 599, loss: 0.170\n",
            "Epoch 600. Time: 1.214, Train loss: 0.170\n",
            "Epoch 604, loss: 0.169\n",
            "Epoch 605. Time: 1.248, Train loss: 0.169\n",
            "Epoch 609, loss: 0.169\n",
            "Epoch 610. Time: 1.226, Train loss: 0.169\n",
            "Epoch 614, loss: 0.168\n",
            "Epoch 615. Time: 1.221, Train loss: 0.168\n",
            "Epoch 619, loss: 0.167\n",
            "Epoch 620. Time: 1.268, Train loss: 0.167\n",
            "Epoch 624, loss: 0.167\n",
            "Epoch 625. Time: 1.231, Train loss: 0.167\n",
            "Epoch 629, loss: 0.166\n",
            "Epoch 630. Time: 1.677, Train loss: 0.166\n",
            "Epoch 634, loss: 0.166\n",
            "Epoch 635. Time: 1.241, Train loss: 0.165\n",
            "Epoch 639, loss: 0.165\n",
            "Epoch 640. Time: 2.034, Train loss: 0.165\n",
            "Epoch 644, loss: 0.164\n",
            "Epoch 645. Time: 1.210, Train loss: 0.164\n",
            "Epoch 649, loss: 0.164\n",
            "Epoch 650. Time: 1.768, Train loss: 0.164\n",
            "Epoch 654, loss: 0.163\n",
            "Epoch 655. Time: 1.224, Train loss: 0.163\n",
            "Epoch 659, loss: 0.163\n",
            "Epoch 660. Time: 1.239, Train loss: 0.163\n",
            "Epoch 664, loss: 0.162\n",
            "Epoch 665. Time: 1.217, Train loss: 0.162\n",
            "Epoch 669, loss: 0.162\n",
            "Epoch 670. Time: 1.247, Train loss: 0.161\n",
            "Epoch 674, loss: 0.161\n",
            "Epoch 675. Time: 1.206, Train loss: 0.161\n",
            "Epoch 679, loss: 0.160\n",
            "Epoch 680. Time: 1.233, Train loss: 0.160\n",
            "Epoch 684, loss: 0.160\n",
            "Epoch 685. Time: 1.213, Train loss: 0.160\n",
            "Epoch 689, loss: 0.159\n",
            "Epoch 690. Time: 1.232, Train loss: 0.159\n",
            "Epoch 694, loss: 0.159\n",
            "Epoch 695. Time: 1.220, Train loss: 0.159\n",
            "Epoch 699, loss: 0.158\n",
            "Epoch 700. Time: 1.222, Train loss: 0.158\n",
            "Epoch 704, loss: 0.158\n",
            "Epoch 705. Time: 1.231, Train loss: 0.158\n",
            "Epoch 709, loss: 0.157\n",
            "Epoch 710. Time: 1.204, Train loss: 0.157\n",
            "Epoch 714, loss: 0.157\n",
            "Epoch 715. Time: 1.371, Train loss: 0.157\n",
            "Epoch 719, loss: 0.156\n",
            "Epoch 720. Time: 1.215, Train loss: 0.156\n",
            "Epoch 724, loss: 0.156\n",
            "Epoch 725. Time: 1.628, Train loss: 0.156\n",
            "Epoch 729, loss: 0.155\n",
            "Epoch 730. Time: 1.198, Train loss: 0.155\n",
            "Epoch 734, loss: 0.155\n",
            "Epoch 735. Time: 1.959, Train loss: 0.155\n",
            "Epoch 739, loss: 0.155\n",
            "Epoch 740. Time: 1.209, Train loss: 0.154\n",
            "Epoch 744, loss: 0.154\n",
            "Epoch 745. Time: 1.937, Train loss: 0.154\n",
            "Epoch 749, loss: 0.154\n",
            "Epoch 750. Time: 1.207, Train loss: 0.154\n",
            "Epoch 754, loss: 0.153\n",
            "Epoch 755. Time: 1.701, Train loss: 0.153\n",
            "Epoch 759, loss: 0.153\n",
            "Epoch 760. Time: 1.207, Train loss: 0.153\n",
            "Epoch 764, loss: 0.152\n",
            "Epoch 765. Time: 1.461, Train loss: 0.152\n",
            "Epoch 769, loss: 0.152\n",
            "Epoch 770. Time: 1.207, Train loss: 0.152\n",
            "Epoch 774, loss: 0.152\n",
            "Epoch 775. Time: 1.231, Train loss: 0.151\n",
            "Epoch 779, loss: 0.151\n",
            "Epoch 780. Time: 1.210, Train loss: 0.151\n",
            "Epoch 784, loss: 0.151\n",
            "Epoch 785. Time: 1.199, Train loss: 0.151\n",
            "Epoch 789, loss: 0.150\n",
            "Epoch 790. Time: 1.206, Train loss: 0.150\n",
            "Epoch 794, loss: 0.150\n",
            "Epoch 795. Time: 1.208, Train loss: 0.150\n",
            "Epoch 799, loss: 0.150\n",
            "Epoch 800. Time: 1.201, Train loss: 0.149\n",
            "Epoch 804, loss: 0.149\n",
            "Epoch 805. Time: 1.213, Train loss: 0.149\n",
            "Epoch 809, loss: 0.149\n",
            "Epoch 810. Time: 1.214, Train loss: 0.149\n",
            "Epoch 814, loss: 0.148\n",
            "Epoch 815. Time: 1.215, Train loss: 0.148\n",
            "Epoch 819, loss: 0.148\n",
            "Epoch 820. Time: 1.205, Train loss: 0.148\n",
            "Epoch 824, loss: 0.148\n",
            "Epoch 825. Time: 1.209, Train loss: 0.148\n",
            "Epoch 829, loss: 0.147\n",
            "Epoch 830. Time: 1.210, Train loss: 0.147\n",
            "Epoch 834, loss: 0.147\n",
            "Epoch 835. Time: 1.213, Train loss: 0.147\n",
            "Epoch 839, loss: 0.147\n",
            "Epoch 840. Time: 1.226, Train loss: 0.147\n",
            "Epoch 844, loss: 0.146\n",
            "Epoch 845. Time: 1.225, Train loss: 0.146\n",
            "Epoch 849, loss: 0.146\n",
            "Epoch 850. Time: 1.561, Train loss: 0.146\n",
            "Epoch 854, loss: 0.212\n",
            "Epoch 855. Time: 1.199, Train loss: 0.265\n",
            "Epoch 859, loss: 0.155\n",
            "Epoch 860. Time: 1.800, Train loss: 0.152\n",
            "Epoch 864, loss: 0.149\n",
            "Epoch 865. Time: 1.226, Train loss: 0.148\n",
            "Epoch 869, loss: 0.147\n",
            "Epoch 870. Time: 1.939, Train loss: 0.147\n",
            "Epoch 874, loss: 0.146\n",
            "Epoch 875. Time: 1.182, Train loss: 0.146\n",
            "Epoch 879, loss: 0.145\n",
            "Epoch 880. Time: 1.854, Train loss: 0.145\n",
            "Epoch 884, loss: 0.145\n",
            "Epoch 885. Time: 1.200, Train loss: 0.145\n",
            "Epoch 889, loss: 0.144\n",
            "Epoch 890. Time: 1.664, Train loss: 0.144\n",
            "Epoch 894, loss: 0.144\n",
            "Epoch 895. Time: 1.191, Train loss: 0.144\n",
            "Epoch 899, loss: 0.143"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Проверка модели.**"
      ],
      "metadata": {
        "id": "j5CR0htOXFcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentence(sentence):\n",
        "    ret = []\n",
        "\n",
        "    x = torch.zeros((1, len(sentence)), dtype=int)\n",
        "    \n",
        "    for j, w in enumerate(sentence):\n",
        "        x[0, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])\n",
        "        \n",
        "    o = model(x)\n",
        "\n",
        "    for i in range(len(sentence)):\n",
        "        w = torch.argmax(o[-1, i, :], keepdim=True)\n",
        "        ww = INDEX_TO_CHAR[w]\n",
        "        \n",
        "        if ww == 'none':\n",
        "            break\n",
        "\n",
        "        ret.append(ww)\n",
        "        \n",
        "    return ''.join(ret)"
      ],
      "metadata": {
        "id": "tKz8yib2XEO3"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt_start = 30\n",
        "cnt = 10"
      ],
      "metadata": {
        "id": "RVquCKDu2lvi"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "for ph in text_orig.split('\\n'):\n",
        "    print(ph)\n",
        "    c += 1\n",
        "    if c > cnt: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZc6liaTApSw",
        "outputId": "2ccb0ad3-7fcf-4388-9858-97620fafe60f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "сказка о попе и о работнике его балде\n",
            "жил-был поп,\n",
            "толоконный лоб.\n",
            "пошел поп по базару\n",
            "посмотреть кой-какого товару.\n",
            "навстречу ему балда\n",
            "идет, сам не зная куда.\n",
            "\"что, батька, так рано поднялся?\n",
            "чего ты взыскался?\"\n",
            "поп ему в ответ: \"нужен мне работник:\n",
            "повар, конюх и плотник.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "for ph in text_orig.split('\\n'):\n",
        "    encripted_text = ceaser_cipher(ph.strip(), key=3, lang=\"русский\")  # Зашифрованный текст\n",
        "    etxt = ''.join([c for c in re.sub('\\s+', ' ', encripted_text) if not (c in ['.', ',', '-', ':', '\"', '!', '?'])])\n",
        "    print(predict_sentence(etxt))                                      # Расшифровываем текст обученной моделью                    \n",
        "    c += 1\n",
        "    if c > cnt: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrcJSoNHzjn6",
        "outputId": "0f088cfc-b7cb-4401-a757-e19500308d8e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "плазка о попе и о работнике его балде\n",
            "вдеаыл поп\n",
            "нолоконнюй моб\n",
            "иояел поп по базару\n",
            "иосмотреть койкакого товару\n",
            "иавстречу ему балда\n",
            "идет сам не зная куда\n",
            "сто батька так рано поднялся\n",
            "севн ты взюскался\n",
            "иоп длу в ответ нужен мне работник\n",
            "иовар коных и плотник\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in [2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
        "    print(\"key =\", key, \":\")\n",
        "    encripted_text = ceaser_cipher(\"звездное небо над морем\", key=key, lang=\"русский\")\n",
        "    print(encripted_text)\n",
        "    print(predict_sentence(encripted_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN3lS5UgPIDh",
        "outputId": "5e85de39-a28e-4904-fe1c-60f4cf5d13d5"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key = 2 :\n",
            "йджйёпрж пжгр пвё ортжо\n",
            "ввеебное небо над морем\n",
            "key = 3 :\n",
            "кезкжрсз рздс ргж псузп\n",
            "ивездное небо над морем\n",
            "key = 4 :\n",
            "лёилзсти сиет сдз ртфир\n",
            "идездное небо над морем\n",
            "key = 5 :\n",
            "мжймитуй тйёу теи сухйс\n",
            "иеездное небо над морем\n",
            "key = 6 :\n",
            "нзкнйуфк укжф уёй тфцкт\n",
            "ивездлое небо над морем\n",
            "key = 7 :\n",
            "оилокфхл флзх фжк ухчлу\n",
            "иееждное небо над морем\n",
            "key = 8 :\n",
            "пймплхцм хмиц хзл фцшмф\n",
            "ивеиеное небо над морем\n",
            "key = 9 :\n",
            "ркнрмцчн цнйч цим хчщнх\n",
            "иелодное небо над морем\n",
            "key = 10 :\n",
            "слоснчшо чокш чйн цшъоц\n",
            "иилиднюе небо над морем\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выводы: Модель почти расшифровывает зашифрованный кодом Цезаря с разным смещением текст."
      ],
      "metadata": {
        "id": "_N4B9uc9Bojf"
      }
    }
  ]
}